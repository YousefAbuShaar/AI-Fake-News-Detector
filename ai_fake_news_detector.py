# -*- coding: utf-8 -*-
"""AI Fake News Detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lo_VoNE0pEk4JJh79DaAIlhk5uUO91Vl
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import f1_score, recall_score, precision_score, classification_report, confusion_matrix
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from xgboost import XGBClassifier

print("reading data...")
data = pd.read_csv("Fake News Detection Dataset.csv")
df = data.copy()

df.head()

print(f"Number of rows: {df.shape[0]}")
print(f"Number of columns: {df.shape[1]}")

print("Data Types: ")
print(df.dtypes)

print("Null Values: ")
print(df.isnull().sum())

print("Data Description: ")
df.describe()

for column in df.columns:
  print(f"Unique values for '{column}': {df[column].unique()}")

sns.countplot(x = "Label", data = df)
plt.show()

def print_corr_with(column):
  plt.figure(figsize=(9, 6))
  sns.heatmap(df.corr(), annot=True)
  plt.show()


  column_corr = df.corr()[column].sort_values(ascending=False)

  column_corr = column_corr.abs().sort_values(ascending=False)

  print("absolute correlation with", column)
  print(column_corr)

print_corr_with('Label')

df['Average_Words_Per_Sentence'] = df['Word_Count'] / df['Number_of_Sentence']

print_corr_with('Label')

df['Character_Count'] = df['Average_Word_Length'] / df['Word_Count']

df['Word_Count_Per_Character'] = df['Word_Count'] / df['Character_Count']
df['Word_Count_Per_Character'] = df['Word_Count'] / df['Character_Count']
df['Sentence_Count_Per_Character'] = df['Number_of_Sentence'] / df['Character_Count']
df['Average_Character_Per_Sentence'] = df['Character_Count'] / df['Number_of_Sentence']

print_corr_with('Label')

print("Mean of features values when news are fake")
print(df.loc[df['Label'] == 1].mean())

print("\n\nMean of features values when news are real")
print(df.loc[df['Label'] == 0].mean())

diff = df.loc[df['Label'] == 0].mean() - df.loc[df['Label'] == 1].mean()
print("Difference between mean of features values when news are fake and real:")
print(diff)

sns.pairplot(df, hue='Label')
plt.show()

x = df.drop(columns=["Label", "Average_Character_Per_Sentence"])
y = df['Label']


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)

model = XGBClassifier()

params = {
    'eta': [0.001, 0.01, 0.1, 0.2, 0.3],
    'gamma': [0, 0.25, 0.5, 1.0],
    'max_depth': [int(x) for x in np.linspace(5, 30, num=6)],
    'min_child_weight': [3, 4, 5, 6],
    'subsample': [0.6, 0.7, 0.8]
}

grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring='f1', verbose=1)

grid_search.fit(x_train, y_train)

best_model = grid_search.best_estimator_

print("Best parameters:")
print(grid_search.best_params_)

print("Best model score:")
print(grid_search.best_score_)

y_pred = best_model.predict(x_test)

print("Test accuracy:", accuracy_score(y_test, y_pred))
print("Test F1 score:", f1_score(y_test, y_pred))
print("Test recall score:", recall_score(y_test, y_pred))
print("Test precision score:", precision_score(y_test, y_pred))

print("Classification Report:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

print(cm_norm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_norm, annot=True)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Accuracy will be ignored due to the imbalanced data
# Recall will be used to evaluate the model
recall = recall_score(y_test, y_pred)
print("Recall:", recall)